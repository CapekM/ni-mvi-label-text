{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Coronavirus]</td>\n",
       "      <td>Coronavirus: Royal Mail's Derry staff in stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[US election 2020, Donald Trump]</td>\n",
       "      <td>US election 2020 polls: Who is ahead - Trump o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Personal finance, Companies, Retailing, Coron...</td>\n",
       "      <td>Visa and Mastercard accused of charging 'exces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[United States]</td>\n",
       "      <td>Jeff Bridges: Oscar-winning US actor reveals h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Boris Johnson, Brexit]</td>\n",
       "      <td>Brexit: Have EU-UK trade talks reached a dead ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              labels  \\\n",
       "0                                      [Coronavirus]   \n",
       "1                   [US election 2020, Donald Trump]   \n",
       "2  [Personal finance, Companies, Retailing, Coron...   \n",
       "3                                    [United States]   \n",
       "4                            [Boris Johnson, Brexit]   \n",
       "\n",
       "                                                text  \n",
       "0  Coronavirus: Royal Mail's Derry staff in stand...  \n",
       "1  US election 2020 polls: Who is ahead - Trump o...  \n",
       "2  Visa and Mastercard accused of charging 'exces...  \n",
       "3  Jeff Bridges: Oscar-winning US actor reveals h...  \n",
       "4  Brexit: Have EU-UK trade talks reached a dead ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data.json', encoding=\"utf8\")\n",
    "data = json.load(f)\n",
    "for d in data:\n",
    "    d['text'] = d['title'] + d['text']\n",
    "df = pd.DataFrame(data)\n",
    "del df['website']\n",
    "del df['title']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['labels'])\n",
    "number_of_classes = y.shape[1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(df[\"text\"])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {}\n",
    "for t in mlb.classes_:\n",
    "    tags[t] = 0\n",
    "for l in df['labels']:\n",
    "    for t in l:\n",
    "        tags[t] += 1\n",
    "\n",
    "for t in tags:\n",
    "    tags[t] = (int)(tags[t]*0.2)\n",
    "\n",
    "val_tags = tags.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "X_val = []\n",
    "\n",
    "y_train = np.empty(shape=(0,len(mlb.classes_)),dtype=int)\n",
    "y_test = np.empty(shape=(0,len(mlb.classes_)),dtype=int)\n",
    "y_val = np.empty(shape=(0,len(mlb.classes_)),dtype=int)\n",
    "\n",
    "for x in range(len(X)):\n",
    "    flag = 0\n",
    "    for t in df.iloc[x]['labels']:\n",
    "        if(tags[t] > 0):\n",
    "            flag = 1\n",
    "            break;\n",
    "        elif(val_tags[t] > 0):\n",
    "            flag = 2\n",
    "            break;\n",
    "        \n",
    "    if flag == 1:\n",
    "        for t in df.iloc[x]['labels']:\n",
    "            tags[t] -= 1\n",
    "        X_test.append(df.iloc[x]['text'])\n",
    "        y_test = np.vstack([y_test, y[x]])\n",
    "    elif flag == 2:\n",
    "        for t in df.iloc[x]['labels']:\n",
    "            val_tags[t] -= 1\n",
    "        X_val.append(df.iloc[x]['text'])\n",
    "        y_val = np.vstack([y_val, y[x]])\n",
    "    else:\n",
    "        X_train.append(df.iloc[x]['text'])\n",
    "        y_train = np.vstack([y_train, y[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_y = y.sum(axis = 0)\n",
    "class_weight = {}\n",
    "for x in range(number_of_classes):\n",
    "    class_weight[x] = (1 / sum_y[x])*(len(data))/number_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "max_length = 512 # can be up to 512 for BERT\n",
    "batch_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_input = tokenizer.encode_plus(\n",
    "#                         test_sentence,                      \n",
    "#                         add_special_tokens = True, # add [CLS], [SEP]\n",
    "#                         max_length = max_length_test, # max length of the text that can go to BERT\n",
    "#                         pad_to_max_length = True, # add [PAD] tokens\n",
    "#                         return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "#                         truncation=True,\n",
    "#               )\n",
    "# print('encoded', bert_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(text):\n",
    "  return tokenizer.encode_plus(text, \n",
    "                add_special_tokens = True, # add [CLS], [SEP]\n",
    "                max_length = max_length, # max length of the text that can go to BERT\n",
    "                pad_to_max_length = True, # add [PAD] tokens\n",
    "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "                truncation=True,\n",
    "              )\n",
    "\n",
    "# map to the expected input to TFBertForSequenceClassification\n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n",
    "\n",
    "def encode_examples(text_list, labels):\n",
    "\n",
    "  input_ids_list = []\n",
    "  token_type_ids_list = []\n",
    "  attention_mask_list = []\n",
    "  label_list = []\n",
    "    \n",
    "  for t in text_list:\n",
    "    bert_input = convert_example_to_feature(t)\n",
    "  \n",
    "    input_ids_list.append(bert_input['input_ids'])\n",
    "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "    attention_mask_list.append(bert_input['attention_mask'])\n",
    "\n",
    "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, labels)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_encoded = encode_examples(X_train, y_train).batch(batch_size)\n",
    "ds_test_encoded = encode_examples(X_test, y_test).batch(batch_size)\n",
    "ds_val_encoded = encode_examples(X_val, val).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier', 'dropout_37']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# model initialization\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=number_of_classes) # conda install -c anaconda h5py==2.10\n",
    "\n",
    "# choosing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 501 steps\n",
      "501/501 [==============================] - 6900s 14s/step - loss: 0.7441 - acc: 0.9257\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 3\n",
    "bert_history = model.fit(ds_train_encoded,\n",
    "                         epochs=number_of_epochs,\n",
    "                         validation_data=valid_dataset,\n",
    "                         class_weight=class_weight\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 481s 4s/step - loss: 0.6101 - acc: 0.9357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6100562932122437, 0.93572396]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(ds_test_encoded, verbose=1)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
